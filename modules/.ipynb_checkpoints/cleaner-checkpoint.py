import re
import unicodedata
import inflect
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import contractions
import emoji
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.tokenize import sent_tokenize
import numpy as np
from .vectorizer import Vectorizer


class Cleaner:
    """
    –ö–ª–∞—Å—Å Cleaner –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –ø–µ—Ä–µ–¥ –µ–≥–æ –∞–Ω–∞–ª–∏–∑–æ–º.
    –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è HTML-—Ç–µ–≥–æ–≤, URL, —ç–º–æ–¥–∂–∏, —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤,
    –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —á–∏—Å–µ–ª –≤ —Å–ª–æ–≤–∞, —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç–æ–ø-—Å–ª–æ–≤ –∏ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è, –∞ —Ç–∞–∫–∂–µ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏.
    
    –ú–µ—Ç–æ–¥—ã:
        clean_text(self, input_text): –û—á–∏—â–∞–µ—Ç –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç –æ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∏ –ø—Ä–∏–≤–æ–¥–∏—Ç –µ–≥–æ –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É –≤–∏–¥—É.
        
        emojis_words(self, text): –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —ç–º–æ–¥–∂–∏ –≤ —Å–ª–æ–≤–∞, –∏—Å–ø–æ–ª—å–∑—É—è –∏—Ö —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ.
        
        remove_noise_boilerplate(self, vectorizer, input_text, min_cluster_size=2, num_clusters=3, max_noise_ratio=0.3):
            –§—É–Ω–∫—Ü–∏—è –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è —à—É–º–∞ –∏ —à–∞–±–ª–æ–Ω–Ω—ã—Ö —Ñ—Ä–∞–∑ –∏–∑ —Ç–µ–∫—Å—Ç–∞. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã vectorizer, min_cluster_size, num_clusters –∏ max_noise_ratio
            –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ—á–∏—Å—Ç–∫–∏. (–§—É–Ω–∫—Ü–∏—è –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –≤ –¥–∞–Ω–Ω–æ–º –ø—Ä–∏–º–µ—Ä–µ)
    
    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
        cleaner = Cleaner()
        raw_text = "Some raw text with HTML <html>...</html>, URLs http://example.com, and emojis üòä."
        clean_text = cleaner.clean_text(raw_text)
        print(clean_text)
    """

    def __init__(self):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–ª–∞—Å—Å–∞ Cleaner, –∑–∞–≥—Ä—É–∂–∞—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ä–µ—Å—É—Ä—Å—ã –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏.
        """

        self.model_lemmatizer = WordNetLemmatizer()
        self.tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
        
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
    def clean_text(self, input_text):  
        """
        –û—á–∏—â–∞–µ—Ç –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç –æ—Ç HTML-—Ç–µ–≥–æ–≤, URL, —ç–º–æ–¥–∂–∏, —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤,
        –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —á–∏—Å–ª–∞ –≤ —Å–ª–æ–≤–∞, —É–¥–∞–ª—è–µ—Ç —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è, –∏ –ø—Ä–æ–≤–æ–¥–∏—Ç –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é.

        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            input_text (str): –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            str: –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.
        """
        
        # HTML-—Ç–µ–≥–∏: –ø–µ—Ä–≤—ã–π —à–∞–≥ - —É–¥–∞–ª–∏—Ç—å –∏–∑ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤—Å–µ HTML-—Ç–µ–≥–∏
        clean_text = re.sub('<[^<]+?>', '', input_text)

        # URL –∏ —Å—Å—ã–ª–∫–∏: –¥–∞–ª–µ–µ - —É–¥–∞–ª—è–µ–º –∏–∑ —Ç–µ–∫—Å—Ç–∞ –≤—Å–µ URL –∏ —Å—Å—ã–ª–∫–∏
        clean_text = re.sub(r'http\S+', '', clean_text)

        # –≠–º–æ–¥–∂–∏ –∏ —ç–º–æ—Ç–∏–∫–æ–Ω—ã: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —ç–º–æ–¥–∂–∏ –≤ —Ç–µ–∫—Å—Ç
        # –í–∞–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é –æ–∫—Ä–∞—Å–∫—É –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        clean_text = self.emojis_words(clean_text)

        # –ü—Ä–∏–≤–æ–¥–∏–º –≤—Å–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        clean_text = clean_text.lower()

        # –£–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—Ä–æ–±–µ–ª—ã
        # –¢–∞–∫ –∫–∞–∫ –≤—Å–µ –¥–∞–Ω–Ω—ã–µ —Ç–µ–ø–µ—Ä—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Å–ª–æ–≤–∞–º–∏ - —É–¥–∞–ª–∏–º –ø—Ä–æ–±–µ–ª—ã
        clean_text = re.sub('\s+', ' ', clean_text)

        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å–∏–º–≤–æ–ª–æ–≤ —Å –¥–∏–∞–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º–∏ –∑–Ω–∞–∫–∞–º–∏ –∫ ASCII-—Å–∏–º–≤–æ–ª–∞–º: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é normalize –∏–∑ –º–æ–¥—É–ª—è unicodedata –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–∏–º–≤–æ–ª—ã —Å –¥–∏–∞–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º–∏ –∑–Ω–∞–∫–∞–º–∏ –∫ ASCII-—Å–∏–º–≤–æ–ª–∞–º
        clean_text = unicodedata.normalize('NFKD', clean_text).encode('ascii', 'ignore').decode('utf-8', 'ignore')

        # –†–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è: —Ç–µ–∫—Å—Ç —á–∞—Å—Ç–æ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤—Ä–æ–¥–µ "don't" –∏–ª–∏ "won't", –ø–æ—ç—Ç–æ–º—É —Ä–∞–∑–≤–µ—Ä–Ω—ë–º –ø–æ–¥–æ–±–Ω—ã–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è
        clean_text = contractions.fix(clean_text)

        # –£–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã: –∏–∑–±–∞–≤–ª—è–µ–º—Å—è –æ—Ç –≤—Å–µ–≥–æ, —á—Ç–æ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è "—Å–ª–æ–≤–∞–º–∏"
        clean_text = re.sub('[^a-zA-Z0-9\s]', '', clean_text)

        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —á–∏—Å–ª–∞ –ø—Ä–æ–ø–∏—Å—å—é: 100 –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç—Å—è –≤ "—Å—Ç–æ" (–¥–ª—è –∫–æ–º–ø—å—é—Ç–µ—Ä–∞)
        temp = inflect.engine()
        words = []
        for word in clean_text.split():
            if word.isdigit():
                words.append(temp.number_to_words(word))
            else:
                words.append(word)
        clean_text = ' '.join(words)

        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞: —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ - —ç—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤
        stop_words = set(stopwords.words('english'))
        tokens = word_tokenize(clean_text)
        tokens = [token for token in tokens if token not in stop_words]
        clean_text = ' '.join(tokens)

        # –ó–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è: –¥–∞–ª–µ–µ - —É–¥–∞–ª—è–µ–º –∏–∑ —Ç–µ–∫—Å—Ç–∞ –≤—Å–µ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        clean_text = re.sub(r'[^\w\s]', '', clean_text)
        
        # –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ
        clean_text = ' '.join([self.model_lemmatizer.lemmatize(w) for w in clean_text.split(' ')])
        # –ò –Ω–∞–∫–æ–Ω–µ—Ü - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        return clean_text

    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —ç–º–æ–¥–∂–∏ –≤ —Å–ª–æ–≤–∞
    def emojis_words(self, text):
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —ç–º–æ–¥–∂–∏ –≤ —Ç–µ–∫—Å—Ç–µ –≤ –∏—Ö —Å–ª–æ–≤–µ—Å–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è.

        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            text (str): –¢–µ–∫—Å—Ç –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —ç–º–æ–¥–∂–∏.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            str: –¢–µ–∫—Å—Ç —Å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã–º–∏ —ç–º–æ–¥–∂–∏.
        """
        # –ú–æ–¥—É–ª—å emoji: –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —ç–º–æ–¥–∂–∏ –≤ –∏—Ö —Å–ª–æ–≤–µ—Å–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è
        clean_text = emoji.demojize(text, delimiters=(" ", " "))

        # –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø—É—Ç—ë–º –∑–∞–º–µ–Ω—ã ":" –∏" _", –∞ —Ç–∞–∫ –∂–µ - –ø—É—Ç—ë–º –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ–±–µ–ª–∞ –º–µ–∂–¥—É –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
        clean_text = clean_text.replace(":", "").replace("_", " ")

        return clean_text
    
    def remove_noise_boilerplate(self, 
                                 vectorizer: Vectorizer, 
                                 input_text: str, 
                                 min_cluster_size: int = 2, 
                                 num_clusters: int = 3, 
                                 max_noise_ratio: float = 0.3):
        """
        –§—É–Ω–∫—Ü–∏—è –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è —à—É–º–∞ –∏ —à–∞–±–ª–æ–Ω–Ω—ã—Ö —Ñ—Ä–∞–∑ –∏–∑ —Ç–µ–∫—Å—Ç–∞. –î–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏.

        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            vectorizer: –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ —á–∏—Å–ª–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã.
            input_text (str): –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏.
            min_cluster_size (int): –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.
            num_clusters (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.
            max_noise_ratio (float): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –¥–æ–ø—É—Å—Ç–∏–º–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —à—É–º–∞.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            –¢–∏–ø –≤–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–µ —É–∫–∞–∑–∞–Ω –∏–∑-–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏.
        """

        # –†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: –¥–ª—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ —à–∞–±–ª–æ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏–ª–∏ "—à—É–º–∞" —Å–Ω–∞—á–∞–ª–∞ –Ω–∞–¥–æ –≤—ã–¥–µ–ª–∏—Ç—å –∏–∑ —Ç–µ–∫—Å—Ç–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –±—É–¥–µ–º —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º
        sentences = self.tokenizer.tokenize(input_text)
        
        # –¥–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ —É–∫–∞–∑—ã–≤–∞—é num_clusters, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–≤–Ω—è–µ—Ç—Å—è –∫–æ–ª-–≤—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        if len(sentences) <= num_clusters:
            num_clusters = len(sentences)
        # –¥–ª—è —Å—Ä–µ–¥–Ω–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ —É–∫–∞–∑—ã–≤–∞—é num_clusters, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–≤–Ω—è–µ—Ç—Å—è —Ç—Ä—ë–º
        elif len(sentences) < 12:
            num_clusters = 3
        # –¥–ª—è —Å—Ä–µ–¥–Ω–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ —É–∫–∞–∑—ã–≤–∞—é num_clusters, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–≤–Ω—è–µ—Ç—Å—è –∫–æ–ª-–≤—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –¥–µ–ª—ë–Ω–Ω—ã—Ö –Ω–∞ 4
        else:
            num_clusters = int(len(sentences)/4)
            max_noise_ratio += 0.1
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –º–∞—Ç—Ä–∏—Ü—É —Å–ª–æ–≤–µ—Å–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        embeddings_matrix = vectorizer.text_vectorize(sentences)

        # KMean-–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è: –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è —Ä–∞–∑–º–µ—Å—Ç–∏—Ç—å –ø–æ—Ö–æ–∂–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–æ–±–ª–∏–∑–æ—Å—Ç–∏ –¥—Ä—É–≥ –æ—Ç –¥—Ä—É–≥–∞ 
        kmeans_model = KMeans(n_clusters=num_clusters)
        kmeans_model.fit(embeddings_matrix)
        model_labels = kmeans_model.labels_
        model_centroids = kmeans_model.cluster_centers_
        cluster_sizes = np.bincount(model_labels)

        # –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö "—à—É–º" –∏ —à–∞–±–ª–æ–Ω–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏
        is_noise = np.zeros(num_clusters, dtype=bool)
        for i, centroid in enumerate(model_centroids):
            if cluster_sizes[i] < min_cluster_size:
                # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –∫–æ—Ç–æ—Ä—ã—Ö –º–µ–Ω—å—à–µ, —á–µ–º –ø–æ—Ä–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ - min_cluster_size
                continue
            distances = np.linalg.norm(embeddings_matrix[model_labels == i] - centroid, axis=1)
            median_distance = np.median(distances)
            if np.count_nonzero(distances > median_distance) / cluster_sizes[i] > max_noise_ratio:
                is_noise[i] = True

        # –£–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–Ω—É–∂–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω—ã –∫–∞–∫ "—à—É–º" –∏–ª–∏ —à–∞–±–ª–æ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, —É–¥–∞–ª—è—é—Ç—Å—è
        filtered_sentences = []
        for i, sentence in enumerate(sentences):
            if not is_noise[model_labels[i]]:
                filtered_sentences.append(sentence)

        filtered_text = ' '.join(filtered_sentences)
        return filtered_text